{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from re import sub, compile\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   id                                               text\n",
      "0  125082707389718529  Today very cheap for 2198597 Icemaker.And Free...\n",
      "1  125085987431923713  @fashionNOGuilt haha! tomorrow should be less ...\n",
      "2  125129328446017536                                DAMN YOU !!! @apple\n",
      "3  125165176772247552                      Love love love iOS 5!! @apple\n",
      "4  125184213342367744  Discount Hemp Knots today.Cheap price too.Save...\n"
     ]
    }
   ],
   "source": [
    "# the location for tweets in json format\n",
    "data_dir = \"sanders data\\\\sanders\\\\rawdata\" # you need to change this\n",
    "\n",
    "# define a function that reads read id and text from each json file\n",
    "def read_data(file_name):\n",
    "    with open(file_name, \"r\") as in_file:\n",
    "        status = json.loads(in_file.read())\n",
    "        try:\n",
    "            return [status[\"id\"], status[\"text\"]]\n",
    "        except KeyError:\n",
    "            return [\"\", \"\"]\n",
    "    \n",
    "\n",
    "# the os.lisdtdir method return a list of file name (not full path) in the specified directory\n",
    "# the os.path.join joins a directory path and a file name to make a full file path\n",
    "file_names = list(map(lambda x: os.path.join(data_dir, x), os.listdir(data_dir)))\n",
    "\n",
    "# read id and text\n",
    "data1 = list(map(read_data, file_names))\n",
    "\n",
    "# convert the list of ids and texts to a data frame\n",
    "df_data1 = pd.DataFrame(data1, columns=[\"id\", \"text\"])\n",
    "\n",
    "# convert id from interger to string\n",
    "print(df_data1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data: 592\n",
      "Sample size: 4921\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing data: %d\" % len(df_data1[df_data1[\"id\"] == \"\"]))\n",
    "# remove missing data\n",
    "df_data1 = df_data1[df_data1[\"id\"] != \"\"]\n",
    "print(\"Sample size: %d\" % len(df_data1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   topic  polarity                  id\n",
      "0  apple  positive  126415614616154112\n",
      "1  apple  positive  126404574230740992\n",
      "2  apple  positive  126402758403305474\n",
      "3  apple  positive  126397179614068736\n",
      "4  apple  positive  126395626979196928\n"
     ]
    }
   ],
   "source": [
    "# read the labels\n",
    "df_labels = pd.read_csv(\"sanders data\\\\sanders\\\\corpus.csv\",\n",
    "                        names=[\"topic\", \"polarity\", \"id\"])\n",
    "print(df_labels.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.int64'>\n",
      "<class 'int'>\n",
      "<class 'numpy.int64'>\n",
      "                   id                                               text  \\\n",
      "0  125082707389718529  Today very cheap for 2198597 Icemaker.And Free...   \n",
      "1  125085987431923713  @fashionNOGuilt haha! tomorrow should be less ...   \n",
      "2  125129328446017536                                DAMN YOU !!! @apple   \n",
      "3  125165176772247552                      Love love love iOS 5!! @apple   \n",
      "4  125184213342367744  Discount Hemp Knots today.Cheap price too.Save...   \n",
      "\n",
      "   topic    polarity  \n",
      "0  apple  irrelevant  \n",
      "1  apple     neutral  \n",
      "2  apple    negative  \n",
      "3  apple    positive  \n",
      "4  apple  irrelevant  \n"
     ]
    }
   ],
   "source": [
    "# join labels with texts\n",
    "print(type(df_labels[\"id\"][0]))\n",
    "print(type(df_data1[\"id\"][0]))\n",
    "df_data1[\"id\"] = pd.to_numeric(df_data1[\"id\"])\n",
    "print(type(df_data1[\"id\"][0]))\n",
    "df_data2 = df_data1.merge(df_labels, on=\"id\", how=\"left\")\n",
    "print(df_data2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "microsoft    1304\n",
      "google       1287\n",
      "twitter      1219\n",
      "apple        1111\n",
      "Name: topic, dtype: int64\n",
      "neutral       2244\n",
      "irrelevant    1623\n",
      "negative       548\n",
      "positive       506\n",
      "Name: polarity, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check the distribution of topic and polarity\n",
    "print(df_data2[\"topic\"].value_counts())\n",
    "print(df_data2[\"polarity\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprecessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['today',\n",
       " 'very',\n",
       " 'cheap',\n",
       " 'for',\n",
       " 'icemaker',\n",
       " 'and',\n",
       " 'free',\n",
       " 'shipping',\n",
       " 'for',\n",
       " 'icemaker',\n",
       " 'too']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a custom tokenizer\n",
    "def tokenization(text):\n",
    "    # replace mention \n",
    "    text = sub(\"@[^ ]+\", \" \", text)\n",
    "    # replace hashtags with space\n",
    "    text = sub(\"#[^ ]+\", \" \", text)\n",
    "    # replace RT (retweet) with space\n",
    "    text = sub(\"RT\", \" \", text)\n",
    "    # replace URL with space\n",
    "    text = sub(\"http[^ ]+\", \" \", text)\n",
    "\n",
    "    p = compile(\"[^a-z]\")\n",
    "    # conver the text to lower case and split by non-alphabetic characters\n",
    "    # also remove \"\" due to tokenizing multple spaces\n",
    "    return [token for token in p.split(text.lower()) if token != \"\"]\n",
    "        \n",
    "# test the tokenization function on a single text\n",
    "tokenization(df_data2[\"text\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.94      0.66      0.77       163\n",
      "    negative       0.33      0.51      0.40        55\n",
      "     neutral       0.62      0.66      0.64       224\n",
      "    positive       0.38      0.41      0.39        51\n",
      "\n",
      "    accuracy                           0.62       493\n",
      "   macro avg       0.57      0.56      0.55       493\n",
      "weighted avg       0.67      0.62      0.63       493\n",
      "\n",
      "Fold 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.92      0.75      0.83       163\n",
      "    negative       0.52      0.64      0.57        55\n",
      "     neutral       0.67      0.69      0.68       224\n",
      "    positive       0.33      0.42      0.37        50\n",
      "\n",
      "    accuracy                           0.67       492\n",
      "   macro avg       0.61      0.62      0.61       492\n",
      "weighted avg       0.70      0.67      0.69       492\n",
      "\n",
      "Fold 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.92      0.70      0.79       163\n",
      "    negative       0.48      0.55      0.51        55\n",
      "     neutral       0.64      0.75      0.69       224\n",
      "    positive       0.29      0.24      0.26        50\n",
      "\n",
      "    accuracy                           0.66       492\n",
      "   macro avg       0.58      0.56      0.56       492\n",
      "weighted avg       0.68      0.66      0.66       492\n",
      "\n",
      "Fold 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.86      0.72      0.78       162\n",
      "    negative       0.72      0.51      0.60        55\n",
      "     neutral       0.69      0.86      0.77       225\n",
      "    positive       0.58      0.42      0.49        50\n",
      "\n",
      "    accuracy                           0.73       492\n",
      "   macro avg       0.71      0.63      0.66       492\n",
      "weighted avg       0.74      0.73      0.72       492\n",
      "\n",
      "Fold 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.85      0.81      0.83       162\n",
      "    negative       0.68      0.49      0.57        55\n",
      "     neutral       0.71      0.80      0.75       225\n",
      "    positive       0.37      0.32      0.34        50\n",
      "\n",
      "    accuracy                           0.72       492\n",
      "   macro avg       0.65      0.61      0.62       492\n",
      "weighted avg       0.72      0.72      0.72       492\n",
      "\n",
      "Fold 6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.87      0.86      0.87       162\n",
      "    negative       0.54      0.37      0.44        54\n",
      "     neutral       0.68      0.76      0.72       225\n",
      "    positive       0.33      0.27      0.30        51\n",
      "\n",
      "    accuracy                           0.70       492\n",
      "   macro avg       0.61      0.57      0.58       492\n",
      "weighted avg       0.69      0.70      0.69       492\n",
      "\n",
      "Fold 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.87      0.80      0.83       162\n",
      "    negative       0.50      0.20      0.29        54\n",
      "     neutral       0.66      0.83      0.73       225\n",
      "    positive       0.40      0.27      0.33        51\n",
      "\n",
      "    accuracy                           0.70       492\n",
      "   macro avg       0.61      0.53      0.55       492\n",
      "weighted avg       0.68      0.70      0.68       492\n",
      "\n",
      "Fold 8\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.90      0.85      0.87       162\n",
      "    negative       0.45      0.27      0.34        55\n",
      "     neutral       0.69      0.81      0.75       224\n",
      "    positive       0.36      0.31      0.33        51\n",
      "\n",
      "    accuracy                           0.71       492\n",
      "   macro avg       0.60      0.56      0.57       492\n",
      "weighted avg       0.70      0.71      0.70       492\n",
      "\n",
      "Fold 9\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.86      0.94      0.90       162\n",
      "    negative       0.58      0.35      0.43        55\n",
      "     neutral       0.73      0.79      0.76       224\n",
      "    positive       0.39      0.31      0.35        51\n",
      "\n",
      "    accuracy                           0.74       492\n",
      "   macro avg       0.64      0.60      0.61       492\n",
      "weighted avg       0.72      0.74      0.73       492\n",
      "\n",
      "Fold 10\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.85      0.86      0.86       162\n",
      "    negative       0.30      0.24      0.27        55\n",
      "     neutral       0.65      0.76      0.70       224\n",
      "    positive       0.45      0.20      0.27        51\n",
      "\n",
      "    accuracy                           0.68       492\n",
      "   macro avg       0.56      0.52      0.52       492\n",
      "weighted avg       0.66      0.68      0.66       492\n",
      "\n",
      "Average F1: 0.69\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "# 10-fold cross validation\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "fold = 0\n",
    "f1 = []\n",
    "for train_index, test_index in skf.split(df_data2[\"text\"], df_data2[\"polarity\"]):\n",
    "#for train_index, test_index in skf:\n",
    "    fold += 1\n",
    "    print(\"Fold %d\" % fold)\n",
    "    # partition\n",
    "    train_x, test_x = df_data2[\"text\"].iloc[train_index], df_data2[\"text\"].iloc[test_index]\n",
    "    train_y, test_y = df_data2[\"polarity\"].iloc[train_index], df_data2[\"polarity\"].iloc[test_index]\n",
    "    # vectorize\n",
    "    vectorizer = TfidfVectorizer(tokenizer=tokenization, max_df=0.8, stop_words='english')\n",
    "    X = vectorizer.fit_transform(train_x)\n",
    "    X_test = vectorizer.transform(test_x)\n",
    "    # train model\n",
    "    clf = SGDClassifier(random_state=fold)\n",
    "    clf.fit(X, train_y)\n",
    "    # predict\n",
    "    pred_y = clf.predict(X_test)\n",
    "    # classification results\n",
    "    for line in metrics.classification_report(test_y, pred_y).split(\"\\n\"):\n",
    "        print(line)\n",
    "    f1.append(metrics.f1_score(test_y, pred_y, average='weighted'))\n",
    "print(\"Average F1: %.2f\" % np.mean(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using sentiment lexicon for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and unzip the opinion lexicon from http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html or the files is posted on D2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-faced -1\n",
      "2-faces -1\n",
      "abnormal -1\n",
      "abolish -1\n",
      "abominable -1\n",
      "abominably -1\n"
     ]
    }
   ],
   "source": [
    "# read the lexicon\n",
    "lexicon = dict()\n",
    "\n",
    "# read postive words\n",
    "with open(\"opinion-lexicon-English\\\\negative-words.txt\", \"r\") as in_file:\n",
    "    for line in in_file.readlines():\n",
    "        if not line.startswith(\";\") and line != \"\\n\":\n",
    "            lexicon[line.strip()] = -1\n",
    "\n",
    "# read negative words\n",
    "with open(\"opinion-lexicon-English\\\\positive-words.txt\", \"r\") as in_file:\n",
    "    for line in in_file.readlines():\n",
    "        if not line.startswith(\";\") and line != \"\\n\":\n",
    "            lexicon[line.strip()] = 1\n",
    "\n",
    "# print the top 5 entries\n",
    "for i, (k, v) in enumerate(lexicon.items()):\n",
    "    print(k, v)\n",
    "    if i > 4: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Apple: Siri is amazing!!! I'm in love!\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "# define a function that uses sentiment word voting to classify sentiment\n",
    "def lexicon_classify(text):\n",
    "    score = 0\n",
    "    for token in tokenization(text):\n",
    "        score += lexicon.get(token, 0)\n",
    "    if score > 0: return \"positive\"\n",
    "    elif score <0: return \"negative\"\n",
    "    else: return \"neutral\"\n",
    "    \n",
    "# test the function on a single text\n",
    "text = \"@Apple: Siri is amazing!!! I'm in love!\"\n",
    "print(text)\n",
    "print(lexicon_classify(\"@Apple: Siri is amazing!!! I'm in love!\"))\n",
    "\n",
    "# alternatively, you can return the score instead of a class using the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     3867\n",
       "negative     548\n",
       "positive     506\n",
       "Name: polarity2, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# before using the lexicon for classification, combine \"neutral\" and \"irrelevant\"\n",
    "df_data2[\"polarity2\"] = df_data2[\"polarity\"].apply(lambda x: \"neutral\" if x==\"irrelevant\" else x)\n",
    "df_data2[\"polarity2\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     3208\n",
       "positive    1056\n",
       "negative     657\n",
       "Name: lex_polarity, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classify using the lexicon\n",
    "df_data2[\"lex_polarity\"] = df_data2[\"text\"].apply(lexicon_classify)\n",
    "df_data2[\"lex_polarity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.38      0.46      0.41       548\n",
      "     neutral       0.88      0.73      0.80      3867\n",
      "    positive       0.29      0.59      0.39       506\n",
      "\n",
      "    accuracy                           0.69      4921\n",
      "   macro avg       0.52      0.59      0.53      4921\n",
      "weighted avg       0.77      0.69      0.72      4921\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# measure the classification performance\n",
    "for line in metrics.classification_report(df_data2[\"polarity2\"], df_data2[\"lex_polarity\"]).split(\"\\n\"):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.29      0.56      0.39        55\n",
      "     neutral       0.89      0.71      0.79       387\n",
      "    positive       0.29      0.45      0.36        51\n",
      "\n",
      "    accuracy                           0.67       493\n",
      "   macro avg       0.49      0.57      0.51       493\n",
      "weighted avg       0.76      0.67      0.70       493\n",
      "\n",
      "Fold 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.67      0.57        55\n",
      "     neutral       0.88      0.84      0.86       387\n",
      "    positive       0.40      0.38      0.39        50\n",
      "\n",
      "    accuracy                           0.78       492\n",
      "   macro avg       0.59      0.63      0.61       492\n",
      "weighted avg       0.79      0.78      0.78       492\n",
      "\n",
      "Fold 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.51      0.60        55\n",
      "     neutral       0.86      0.90      0.88       387\n",
      "    positive       0.23      0.22      0.23        50\n",
      "\n",
      "    accuracy                           0.79       492\n",
      "   macro avg       0.60      0.54      0.57       492\n",
      "weighted avg       0.78      0.79      0.78       492\n",
      "\n",
      "Fold 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.49      0.57        55\n",
      "     neutral       0.87      0.93      0.90       387\n",
      "    positive       0.46      0.38      0.42        50\n",
      "\n",
      "    accuracy                           0.83       492\n",
      "   macro avg       0.68      0.60      0.63       492\n",
      "weighted avg       0.81      0.83      0.82       492\n",
      "\n",
      "Fold 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.55      0.63        55\n",
      "     neutral       0.88      0.95      0.91       387\n",
      "    positive       0.48      0.32      0.39        50\n",
      "\n",
      "    accuracy                           0.84       492\n",
      "   macro avg       0.70      0.61      0.64       492\n",
      "weighted avg       0.82      0.84      0.83       492\n",
      "\n",
      "Fold 6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.37      0.48        54\n",
      "     neutral       0.84      0.94      0.89       387\n",
      "    positive       0.36      0.24      0.29        51\n",
      "\n",
      "    accuracy                           0.80       492\n",
      "   macro avg       0.62      0.51      0.55       492\n",
      "weighted avg       0.77      0.80      0.78       492\n",
      "\n",
      "Fold 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.55      0.20      0.30        54\n",
      "     neutral       0.82      0.94      0.88       387\n",
      "    positive       0.36      0.24      0.29        51\n",
      "\n",
      "    accuracy                           0.78       492\n",
      "   macro avg       0.58      0.46      0.49       492\n",
      "weighted avg       0.75      0.78      0.75       492\n",
      "\n",
      "Fold 8\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.44      0.22      0.29        55\n",
      "     neutral       0.83      0.93      0.88       386\n",
      "    positive       0.50      0.31      0.39        51\n",
      "\n",
      "    accuracy                           0.79       492\n",
      "   macro avg       0.59      0.49      0.52       492\n",
      "weighted avg       0.75      0.79      0.76       492\n",
      "\n",
      "Fold 9\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.58      0.25      0.35        55\n",
      "     neutral       0.83      0.94      0.88       386\n",
      "    positive       0.44      0.27      0.34        51\n",
      "\n",
      "    accuracy                           0.79       492\n",
      "   macro avg       0.62      0.49      0.52       492\n",
      "weighted avg       0.76      0.79      0.77       492\n",
      "\n",
      "Fold 10\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.39      0.22      0.28        55\n",
      "     neutral       0.83      0.95      0.89       386\n",
      "    positive       0.44      0.16      0.23        51\n",
      "\n",
      "    accuracy                           0.79       492\n",
      "   macro avg       0.55      0.44      0.47       492\n",
      "weighted avg       0.74      0.79      0.75       492\n",
      "\n",
      "Average F1: 0.77\n"
     ]
    }
   ],
   "source": [
    "# use svm for the 3-class classification\n",
    "# 10-fold cross validation\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "fold = 0\n",
    "\n",
    "# a container for f1 score\n",
    "f1 = []\n",
    "for train_index, test_index in skf.split(df_data2['text'], df_data2['polarity2']):\n",
    "    fold += 1\n",
    "    print(\"Fold %d\" % fold)\n",
    "    # partition\n",
    "    train_x, test_x = df_data2[\"text\"].loc[train_index], df_data2[\"text\"].loc[test_index]\n",
    "    train_y, test_y = df_data2[\"polarity2\"].loc[train_index], df_data2[\"polarity2\"].loc[test_index]\n",
    "    # vectorize\n",
    "    vectorizer = TfidfVectorizer(tokenizer=tokenization, max_df=0.8, stop_words='english')\n",
    "    X = vectorizer.fit_transform(train_x)\n",
    "    X_test = vectorizer.transform(test_x)\n",
    "    # train model\n",
    "    clf = SGDClassifier(random_state=fold)\n",
    "    clf.fit(X, train_y)\n",
    "    # predict\n",
    "    pred_y = clf.predict(X_test)\n",
    "    # classification results\n",
    "    for line in metrics.classification_report(test_y, pred_y).split(\"\\n\"):\n",
    "        print(line)\n",
    "    f1.append(metrics.f1_score(test_y, pred_y, average='weighted'))\n",
    "print(\"Average F1: %.2f\" % np.mean(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifying neutral is easy. Let's take a look at the binary (positive vs. negative) classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-eef42dea7f96>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_data3[\"polarity2\"] = df_data3[\"polarity2\"].map({\"positive\":1, \"negative\":0})\n"
     ]
    }
   ],
   "source": [
    "# subset data\n",
    "df_data3 = df_data2[df_data2[\"polarity2\"] != \"neutral\"]\n",
    "df_data3[\"polarity2\"].value_counts()\n",
    "df_data3[\"polarity2\"] = df_data3[\"polarity2\"].map({\"positive\":1, \"negative\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "      <th>polarity</th>\n",
       "      <th>polarity2</th>\n",
       "      <th>lex_polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>125129328446017536</td>\n",
       "      <td>DAMN YOU !!! @apple</td>\n",
       "      <td>apple</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>125165176772247552</td>\n",
       "      <td>Love love love iOS 5!! @apple</td>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>125202037293064192</td>\n",
       "      <td>RT @gdcurry: Really @Apple?  What have you don...</td>\n",
       "      <td>apple</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>125223685194915840</td>\n",
       "      <td>@ford should have teamed up with @Apple instea...</td>\n",
       "      <td>apple</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>125224588253741056</td>\n",
       "      <td>#Siri went down for a little while last night....</td>\n",
       "      <td>apple</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                               text  \\\n",
       "2   125129328446017536                                DAMN YOU !!! @apple   \n",
       "3   125165176772247552                      Love love love iOS 5!! @apple   \n",
       "7   125202037293064192  RT @gdcurry: Really @Apple?  What have you don...   \n",
       "14  125223685194915840  @ford should have teamed up with @Apple instea...   \n",
       "15  125224588253741056  #Siri went down for a little while last night....   \n",
       "\n",
       "    topic  polarity  polarity2 lex_polarity  \n",
       "2   apple  negative          0     negative  \n",
       "3   apple  positive          1     positive  \n",
       "7   apple  negative          0      neutral  \n",
       "14  apple  negative          0     positive  \n",
       "15  apple  negative          0     negative  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.85      0.80        55\n",
      "           1       0.82      0.71      0.76        51\n",
      "\n",
      "    accuracy                           0.78       106\n",
      "   macro avg       0.79      0.78      0.78       106\n",
      "weighted avg       0.79      0.78      0.78       106\n",
      "\n",
      "Fold 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.96      0.84        55\n",
      "           1       0.94      0.65      0.77        51\n",
      "\n",
      "    accuracy                           0.81       106\n",
      "   macro avg       0.84      0.81      0.80       106\n",
      "weighted avg       0.84      0.81      0.81       106\n",
      "\n",
      "Fold 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.80      0.72        55\n",
      "           1       0.72      0.55      0.62        51\n",
      "\n",
      "    accuracy                           0.68       106\n",
      "   macro avg       0.69      0.67      0.67       106\n",
      "weighted avg       0.69      0.68      0.67       106\n",
      "\n",
      "Fold 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.82      0.83        55\n",
      "           1       0.81      0.84      0.83        51\n",
      "\n",
      "    accuracy                           0.83       106\n",
      "   macro avg       0.83      0.83      0.83       106\n",
      "weighted avg       0.83      0.83      0.83       106\n",
      "\n",
      "Fold 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.71      0.75        55\n",
      "           1       0.71      0.80      0.75        50\n",
      "\n",
      "    accuracy                           0.75       105\n",
      "   macro avg       0.76      0.75      0.75       105\n",
      "weighted avg       0.76      0.75      0.75       105\n",
      "\n",
      "Fold 6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.82      0.83        55\n",
      "           1       0.80      0.82      0.81        50\n",
      "\n",
      "    accuracy                           0.82       105\n",
      "   macro avg       0.82      0.82      0.82       105\n",
      "weighted avg       0.82      0.82      0.82       105\n",
      "\n",
      "Fold 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.65      0.71        55\n",
      "           1       0.68      0.80      0.73        50\n",
      "\n",
      "    accuracy                           0.72       105\n",
      "   macro avg       0.73      0.73      0.72       105\n",
      "weighted avg       0.73      0.72      0.72       105\n",
      "\n",
      "Fold 8\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.73      0.71        55\n",
      "           1       0.69      0.66      0.67        50\n",
      "\n",
      "    accuracy                           0.70       105\n",
      "   macro avg       0.69      0.69      0.69       105\n",
      "weighted avg       0.69      0.70      0.69       105\n",
      "\n",
      "Fold 9\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.69      0.65        54\n",
      "           1       0.63      0.57      0.60        51\n",
      "\n",
      "    accuracy                           0.63       105\n",
      "   macro avg       0.63      0.63      0.63       105\n",
      "weighted avg       0.63      0.63      0.63       105\n",
      "\n",
      "Fold 10\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.70      0.67        54\n",
      "           1       0.64      0.57      0.60        51\n",
      "\n",
      "    accuracy                           0.64       105\n",
      "   macro avg       0.64      0.64      0.64       105\n",
      "weighted avg       0.64      0.64      0.64       105\n",
      "\n",
      "Average F1: 0.72\n"
     ]
    }
   ],
   "source": [
    "# 10-fold cross validation\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "\n",
    "fold = 0\n",
    "\n",
    "# a container for f1 score\n",
    "f1 = []\n",
    "for train_index, test_index in skf.split(df_data3['text'], df_data3['polarity2']):\n",
    "    fold += 1\n",
    "    print(\"Fold %d\" % fold)\n",
    "    # partition\n",
    "    train_x, test_x = df_data3[\"text\"].iloc[train_index], df_data3[\"text\"].iloc[test_index]\n",
    "    train_y, test_y = df_data3[\"polarity2\"].iloc[train_index], df_data3[\"polarity2\"].iloc[test_index]\n",
    "    # vectorize\n",
    "    vectorizer = TfidfVectorizer(tokenizer=tokenization, max_df=0.8, stop_words='english')\n",
    "    X = vectorizer.fit_transform(train_x)\n",
    "    X_test = vectorizer.transform(test_x)\n",
    "    # train model\n",
    "    clf = SGDClassifier()\n",
    "    clf.fit(X, train_y)\n",
    "    # predict\n",
    "    pred_y = clf.predict(X_test)\n",
    "    # classification results\n",
    "    for line in metrics.classification_report(test_y, pred_y).split(\"\\n\"):\n",
    "        print(line)\n",
    "    f1.append(metrics.f1_score(test_y, pred_y))\n",
    "print(\"Average F1: %.2f\" % np.mean(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We then try three methods for dimension reduction. 1. RFE feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenization, max_df=0.8, stop_words='english')\n",
    "X = vectorizer.fit_transform(df_data3[\"text\"])\n",
    "# partition: train/test = 80/20\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, df_data3[\"polarity2\"], test_size=0.2, stratify=df_data3[\"polarity2\"], random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RFECV(cv=StratifiedKFold(n_splits=2, random_state=None, shuffle=False),\n",
       "      estimator=SGDClassifier(), scoring='f1')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate all features\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "clf = SGDClassifier()\n",
    "rfecv = RFECV(estimator=clf, step=1, cv=StratifiedKFold(n_splits=2), scoring=\"f1\")\n",
    "rfecv.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=vectorizer.get_feature_names()\n",
    "selected_terms = [x for (x, y) in zip(columns, rfecv.support_) if y==True]\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenization, max_df=0.8, stop_words='english', vocabulary=selected_terms)\n",
    "X = vectorizer.fit_transform(df_data3[\"text\"])\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, df_data3[\"polarity2\"], test_size=0.2, stratify=df_data3[\"polarity2\"], random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.76      0.76       110\n",
      "           1       0.74      0.74      0.74       101\n",
      "\n",
      "    accuracy                           0.75       211\n",
      "   macro avg       0.75      0.75      0.75       211\n",
      "weighted avg       0.75      0.75      0.75       211\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = SGDClassifier()\n",
    "clf.fit(train_x, train_y)\n",
    "pred_y = clf.predict(test_x)\n",
    "# classification results\n",
    "for line in metrics.classification_report(test_y, pred_y).split(\"\\n\"):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Using just sentiment terms in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.87      0.75       110\n",
      "           1       0.78      0.50      0.61       101\n",
      "\n",
      "    accuracy                           0.69       211\n",
      "   macro avg       0.72      0.68      0.68       211\n",
      "weighted avg       0.71      0.69      0.68       211\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = lexicon.keys()\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenization, max_df=0.8, stop_words='english', vocabulary=vocab)\n",
    "X = vectorizer.fit_transform(df_data3[\"text\"])\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, df_data3[\"polarity2\"], test_size=0.2, stratify=df_data3[\"polarity2\"], random_state=123)\n",
    "#test_x3 = keep_sentiment_terms(test_x)\n",
    "clf = SGDClassifier()\n",
    "clf.fit(train_x, train_y)\n",
    "pred_y = clf.predict(test_x)\n",
    "# classification results\n",
    "for line in metrics.classification_report(test_y, pred_y).split(\"\\n\"):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2468\n",
      "[4.71367798e-03 3.60271771e-03 3.51245306e-03 ... 1.94535059e-36\n",
      " 6.74341370e-37 3.42949719e-37]\n",
      "639\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.75      0.74       110\n",
      "           1       0.71      0.69      0.70       101\n",
      "\n",
      "    accuracy                           0.72       211\n",
      "   macro avg       0.72      0.72      0.72       211\n",
      "weighted avg       0.72      0.72      0.72       211\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#X_std = StandardScaler().fit_transform(X) # you need to do standardization, since pca is sensitive to the relative scaling of the original variables\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenization, max_df=0.8, stop_words='english')\n",
    "X = vectorizer.fit_transform(df_data3[\"text\"]).todense()\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X = StandardScaler().fit_transform(X)\n",
    "print(len(X[0]))\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(svd_solver='randomized',whiten=True).fit(X)\n",
    "print(pca.explained_variance_ratio_)\n",
    "sumofvariance=0.0\n",
    "n_components = 0\n",
    "for item in pca.explained_variance_ratio_:\n",
    "    sumofvariance += item\n",
    "    n_components+=1\n",
    "    if sumofvariance>=0.9:\n",
    "        break\n",
    "print(n_components)\n",
    "pca = PCA(n_components=n_components, svd_solver='randomized',whiten=True).fit(X)\n",
    "X_train_pca = pca.transform(X)\n",
    "train_x, test_x, train_y, test_y = train_test_split(X_train_pca, df_data3[\"polarity2\"], test_size=0.2, stratify=df_data3[\"polarity2\"], random_state=123)\n",
    "clf = SGDClassifier()\n",
    "clf.fit(train_x, train_y)\n",
    "pred_y = clf.predict(test_x)\n",
    "for line in metrics.classification_report(test_y, pred_y).split(\"\\n\"):\n",
    "    print(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
